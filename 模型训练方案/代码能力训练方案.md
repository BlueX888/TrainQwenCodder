# Qwen2.5-Coder-0.5B 基于 GRPO 的 Phaser3 代码生成强化训练方案

## 1. 目标

基于GRPO强化Qwen2.5-Coder-0.5B生成Phaser3代码的能力。

## 2. 整体方案架构

### 2.1 技术路线概览

整体训练流程分为四个阶段依次执行：**基础设施准备 → SFT 冷启动 → GRPO 强化学习 → 评估与部署**。各阶段紧密衔接，前一阶段的产出作为后一阶段的输入。

### 2.2 阶段零：基础设施准备

本阶段为后续训练搭建必要的工具链和数据基础，包含三项核心工作：

**Phaser3 API 索引构建**：解析 Phaser3 官方文档，提取所有类、方法、属性的签名和参数类型，构建可检索的 API 索引库。该索引用于教师模型 Prompt 的上下文注入，以及 GRPO 阶段 API 准确率的校验。

**Prompt 种子库设计**：设计 2000+ 条覆盖 Phaser3 各模块（Scene、Physics、Input、Animations、Tilemap、Particles 等）的代码生成 Prompt，按基础（40%）、中等（40%）、困难（20%）三个难度等级分布，确保训练数据的多样性和完整性。

**验证环境搭建**：配置 ESLint 规则（声明 Phaser 全局变量）、Babel AST 分析工具、Phaser.HEADLESS 无渲染运行环境，为后续的自动化质量过滤和奖励计算提供基础设施。

### 2.3 阶段一：SFT 冷启动

本阶段通过教师模型蒸馏获取高质量训练数据，对基座模型进行监督微调，使其具备基本的Phaser3代码生成能力。

**数据蒸馏**：将 Prompt 种子库输入教师模型（Claude/GPT），每条 Prompt 生成 3 个不同实现版本。生成时根据 Prompt 关键词动态注入相关 Phaser3 API 文档作为上下文，并约束输出格式和代码规范。此步骤产出约 6000 条候选数据。

**三层质量过滤**：6000 条候选数据
    │
    ├─> L1: 语法与基础规范检查 
    │
    ├─> L2: API 语义与领域知识验证
    │
    ├─> L3: 运行时正确性验证
    │
    ├─> L4: 功能匹配度验证
    │
    ├─> L5: 多样性与质量综合筛选
    │
    └─> L6: 人工抽检与校准
        └─> 最终数据集: 3500 条蒸馏 + 1000 条官方 = 4500 条

**数据整合与微调**：对过滤后的数据进行去重和多样性筛选，并补充约 1000 条来自 Phaser 官方示例和 GitHub 开源项目的代码，最终形成约 4500 条的 SFT 数据集。使用该数据集对 Qwen2.5-Coder-0.5B 进行监督微调，产出 SFT 基础模型。

### 2.4 阶段二：GRPO 强化学习

本阶段使用 GRPO 算法对 SFT 模型进行强化训练，通过多维度奖励信号和组内相对比较，进一步提升代码生成质量。

**核心机制**：GRPO摒弃传统PPO的Critic 模型，改用组内相对比较计算优势函数。对于每个训练 Prompt，模型采样生成 G=8 组输出，分别计算每组的奖励值，然后计算组内均值和标准差，将每个输出的奖励标准化为优势值。优势为正的输出被强化，为负的被抑制。这种方法无需额外的价值网络，计算开销低，适合小参数模型。

**奖励函数设计**：

核心架构：总奖励 = 结构化计划奖励 (15%) + 代码奖励 (85%)。结构化计划仅用于对齐与一致性监督，避免长思维链刷分；推理阶段可关闭该输出。

#### 一、结构化计划奖励（3个维度，15%）

**计划格式**：采用固定前缀的纯文本格式，通过宽松解析容错格式瑕疵；按难度动态调整要求；重点考核计划与代码的一致性。
```
[PLAN]
REQ: <一句话需求摘要>
API: <API1>, <API2>, <API3>
STEPS:
1. <步骤1>
2. <步骤2>
3. <步骤3>
[/PLAN]
```

**1. 结构完整性（30%）**

- 必备字段齐全且非空
- steps 至少 2-3 步（按 Prompt 难度动态）

**2. 需求-API一致性（20%）**

- 需求关键词与 API 选择匹配
- API 在索引库中存在
- 仅评估计划合理性，不重复计算代码 API 准确率

**3. 计划-代码一致性（50%，最关键）**

- **正向一致性**：计划中提到的 API，代码中必须使用
- **步骤体现**：计划 steps 在代码结构中可映射（preload/create/update 或等价流程）
- **逆向一致性**：代码中的核心 API，计划中应该有提及
- 防止模型"说一套做一套"

#### 二、代码奖励（5个维度，85%）

**1. 功能完整性（30%）**：以可执行断言为主（HEADLESS + 场景状态检查/AST 规则）；关键词匹配或小模型评估仅作兜底

**2. API准确率（25%）**：AST 对照索引，校验 API 存在性、参数类型与使用场景

**3. 运行时正确性（20%）**：生命周期完整、无崩溃、稳定运行，必要时记录性能上限

**4. 代码质量（15%）**：命名规范、最佳实践、冗余控制

**5. 格式规范（10%）**：结构完整、代码风格一致

**门控与归一化**：

- 所有子奖励归一化到 [0,1] 后再加权，避免尺度漂移
- 语法错误或 AST 解析失败 → 代码奖励置 0（计划奖励不抬分）
- 运行时崩溃 → 代码奖励上限 0.2（或直接置 0，按迭代策略调整）
- 计划缺失/格式不合规 → 仅取消计划奖励，不影响代码奖励

**Reward Hacking 防御**：为防止模型找到绕过奖励的捷径，设置多项约束：最小代码长度/最少 API 数按难度动态设定，不对简单需求硬惩罚；低于阈值采用线性扣分；相似度超过 0.9 的输出施加多样性惩罚；有效代码行占比不低于 70%（防止注释填充）。

**KL 散度控制**：设置自适应的 β 系数控制策略更新幅度。当 KL 散度过大时增大 β 加强约束，防止模型偏离过远导致模式崩坏；当 KL 散度过小时减小 β，鼓励模型探索更优策略。

### 2.5 阶段三：评估

**数据集与设置**：保留 300 条测试 Prompt，按模块/难度分层抽样，含 20% OOD（跨模块组合或更长约束）样本；所有模型使用相同解码参数与随机种子，输出总体与分模块结果。

**自动评估**：Pass@1、Pass@8、API 调用准确率（AST 对照索引）、运行时稳定性（HEADLESS 生命周期/崩溃率）、结构完整率（Phaser.Game 配置 + Scene）。门槛为 Pass@1 ≥45%（目标 ≥50%）、Pass@8 ≥85%、API ≥90%、崩溃率 ≤5%、结构完整率 ≥95%。

**人工评估**：分层抽取 200 条，双人盲评并计算一致性；维度为功能完整性（0-2）、代码质量（0-2）、最佳实践遵循（0-1）。目标平均分 ≥3.5/5 且一致性 κ ≥0.6。

**对比与消融**：对比原始 Qwen2.5-Coder-0.5B、SFT-only、Qwen2.5-Coder-7B；做奖励项消融（去掉运行时/API/一致性）验证增益来源。




### 2.6 关键技术选型

| 环节       | 技术方案              | 选型理由                               |
| ---------- | --------------------- | -------------------------------------- |
| 基座模型   | Qwen2.5-Coder-0.5B    | 代码专精、参数量小、适合强化学习迭代   |
| 教师模型   | Claude / GPT-5        | 代码生成质量高、Phaser3 知识丰富       |
| SFT 框架   | LLaMA-Factory         | 支持多种微调方法、配置灵活             |
| GRPO 实现  | VeRL + 自定义奖励模块 | 框架成熟、奖励函数接入灵活             |
| 运行时验证 | Phaser.HEADLESS       | 无渲染开销（~80ms/样本）、真实引擎验证 |
| API 分析   | Babel AST             | 精确的代码结构解析、避免正则误判       |

```plain
┌─────────────────────────────────────────────────────────────────┐
│                        第一阶段：SFT 冷启动                       │
│                                                                 │
│   Prompt 种子库 ──→ 大模型蒸馏+收集高质量代码 ──→ 自动过滤 ──→ SFT 微调
│     (N条)      (Claude/GPT-5)   (HEADLESS)    (N条)             │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                      第二阶段：GRPO 强化学习                      │
│                                                                 │
│   Prompt ──→ 模型生成 ──→ 奖励计算 ──→ 相对优势 ──→ 策略更新       │
│              (G=8组)     (多维度)     (组内比较)                  │
└─────────────────────────────────────────────────────────────────┘
```

 
