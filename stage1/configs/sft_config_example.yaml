# LLaMA-Factory SFT 配置文件
# 用于 Qwen2.5-Coder-0.5B 的 Phaser3 代码生成微调

### Model
model_name_or_path: Qwen/Qwen2.5-Coder-0.5B

### Method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.05

### Dataset
# 需要在 LLaMA-Factory 的 data/dataset_info.json 中注册数据集
# 或使用绝对路径指定数据集文件
dataset: phaser3_sft
# 也可以使用自定义路径:
# dataset_dir: /Users/admin/Desktop/TrainQwenCodder/stage1/data/sft_dataset
template: qwen
cutoff_len: 2048
max_samples: 10000
overwrite_cache: true
preprocessing_num_workers: 16

### Output
output_dir: stage1/outputs/qwen_coder_0.5b_sft
logging_steps: 10
save_steps: 500
save_total_limit: 3
plot_loss: true
overwrite_output_dir: true

### Train
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 5.0e-5
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### Eval
val_size: 0.05
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 500

### Optional: DeepSpeed (uncomment if needed)
# deepspeed: ds_config.json

### Optional: Quantization for memory efficiency (uncomment if needed)
# quantization_bit: 4
# quantization_method: bitsandbytes

---
# 数据集注册信息 (添加到 LLaMA-Factory 的 data/dataset_info.json)
#
# "phaser3_sft": {
#   "file_name": "/Users/admin/Desktop/TrainQwenCodder/stage1/data/sft_dataset/train.jsonl",
#   "columns": {
#     "prompt": "instruction",
#     "query": "input",
#     "response": "output"
#   }
# }
