# 阶段三：评估（详细实施文档）

> 适用范围：`Qwen2.5-Coder-0.5B 基于 GRPO 的 Phaser3 代码生成强化训练方案` 的 **2.5 阶段三：评估**  
> 本文目标：建立一套**可复现、可分层分析、可回放样本**的评估体系，用于比较 Base / SFT / GRPO（含消融）模型的改进幅度与退化风险。

---

## 0. 本阶段产出（DoD）

阶段三结束时应具备以下可交付物：

1) **固定评估集（300 条）**  
   - 分层抽样（模块×难度），含 20% OOD  
   - JSONL 格式，包含 `must_use_apis/eval_hints` 等可验证信息

2) **自动评估 Harness**  
   - 支持 Pass@1 / Pass@8  
   - 支持 API 命中率、结构完整率、HEADLESS 崩溃率等指标  
   - 所有模型使用**相同解码参数与随机种子**  
   - 每条样本保存生成结果与 validator 输出，便于回放/定位问题

3) **评估报告（可复现）**  
   - 总体指标 + 分模块/分难度/分 OOD 指标  
   - 失败原因 TopN（parse/lint/api/runtime/结构缺失/必用 API 未命中）  
   - 与基线对比表（Base vs SFT vs GRPO）+ 消融对比（可选）

4) **人工评估包**  
   - 200 条抽样集（冻结）+ 盲评表格模板  
   - 评分标准与一致性计算方法（κ）

---

## 1. 前置依赖（阶段零/一/二输出）

- Phaser3 API 索引：`stage0/data/api_index/phaser_api.jsonl`  
- 验证器：`stage0/validator/src/cli.js`（AST 必须可用；ESLint/HEADLESS 尽量启用）  
- 模型产物：Base、SFT-only、GRPO（可选再含 7B 对照与消融模型）

---

## 2. 目录结构与数据约定（建议）

建议新增评估目录（保持冻结与可回放；本仓库对应实现见 `stage3/README.md`）：

```text
stage3/
└─ data/
   └─ eval/
   ├─ prompts_eval_300.jsonl          # 固定评估集（冻结，不随训练变化）
   ├─ prompts_human_200.jsonl         # 人工评估抽样集（冻结）
   ├─ generations/                   # 每个模型的生成结果
   │  └─ <model_name>/
   │     ├─ gen_pass1.jsonl          # 1 次采样结果
   │     └─ gen_pass8.jsonl          # 8 次采样结果（或按 seed 分片）
   ├─ validator/                     # validator 输出缓存（按 code_hash）
   │  └─ <model_name>/
   └─ reports/
      ├─ summary.json                # 总体与分层指标汇总
      ├─ breakdown.csv               # 分模块/难度/OOD 的表格输出
      └─ failures_topn.json          # 失败原因统计
```

> 关键原则：**prompt 冻结、生成可回放、validator 结果可缓存**，避免“跑一次一套结果”。

---

## 3. 评估集构建（300 条，含 20% OOD）

### 3.1 抽样策略（分层）

目标：评估结果可解释、可定位到模块/难度退化点。

建议的分层维度：

- 模块：Scene / GameObjects / Input / Physics / Animations / Tilemap / Camera / Particles（按现有 Prompt 标签）  
- 难度：easy / medium / hard（40/40/20 作为参考，也可与训练集不同）  
- OOD：约 20%（跨模块组合、更多约束、更长 prompt、更强“可验证”要求）

建议做法：

1) 从 `data/prompt_seeds/prompt_seeds.jsonl` 分层采样得到 240 条 IID  
2) 手工/程序化构造 60 条 OOD（建议优先“跨模块组合”而不是完全无关域）

### 3.2 OOD 定义与样例类型

OOD 样例建议包含：

- 跨模块组合：例如 Tilemap + Arcade Physics + Camera follow  
- 更长约束：例如“必须对象池”“必须可配置 seed”“必须输出 __signals__”  
- 组合技能：例如 Input + Tween + UI 状态管理

> OOD 的目标是测试“泛化与组合能力”，不是引入完全新领域（避免评价失真）。

### 3.3 Prompt JSONL Schema（建议）

沿用阶段零/一的字段，至少包含：

```json
{
  "id":"eval_000001",
  "difficulty":"medium",
  "modules":["Physics","Input"],
  "task":"...",
  "constraints":[...],
  "must_use_apis":[...],
  "eval_hints":[...],
  "tags":[...],
  "ood": false
}
```

---

## 4. 自动评估（指标定义与计算方法）

自动评估由两部分组成：

1) **生成**：固定解码参数与随机种子，采样 `k` 个候选（k=1 与 k=8）  
2) **判定**：对每个候选运行 validator（AST/ESLint/HEADLESS），得到 pass/fail 与分项指标

### 4.1 统一解码设置（强约束）

为保证对比公平，所有模型需使用相同设置：

- `temperature/top_p/max_new_tokens` 固定  
- 对每条 prompt 固定 `seed`（Pass@8 用 seed 列表，例如 8 个固定 seed）  
- 输出格式固定为 `plan:` + `code:`（评估期可选择关闭 plan，但需统一对比口径）

建议把解码设置写入 `data/eval/reports/summary.json` 的 `settings` 字段。

### 4.2 Pass@k 定义

对每条 prompt：

- 生成 `k` 个样本 `y1..yk`  
- 定义 `pass(yi)=1` 当且仅当该样本通过“门控判定”（见 4.3）  
- 则 `Pass@k = mean_over_prompts( max_i pass(yi) )`

### 4.3 门控判定（通过/失败）

建议的最小可落地门控（与阶段二奖励一致，便于闭环）：

- `parse_ok=true`
- `api_ok=true`（包含 must-use 命中）
- `structure_ok=true`（见 4.4）
- `runtime_ok=true`（若 HEADLESS 环境稳定；不稳定时可降级为可选门控，但需明确记录）

> 注意：如果 runtime 不作为门控，必须同时报告“runtime 崩溃率”作为独立指标，避免模型靠“静态过关但一跑就炸”刷分。

### 4.4 结构完整率（Structure Rate）

结构完整率用于衡量“Phaser 代码骨架是否完整”，建议由 validator signals 判定：

- 是否创建 `new Phaser.Game(config)`
- 是否存在 `config.scene`
- 是否包含 `preload/create`（按难度可要求 update）

定义：

- `structure_ok`：以上条件全部满足  
- `structure_rate`：在全部生成样本中 `structure_ok` 的比例（可同时统计按难度分层）

### 4.5 API 调用准确率（AST 对照索引）

基于 validator 的 `api_usage.hits/misses`：

- **API miss 率**：`misses / (hits + misses + 1e-9)`  
- **API 准确率**：`1 - API miss 率`

报告口径建议至少给出：

- overall API 准确率  
- must-use 命中率（更关键）  
- Top miss 符号（帮助补齐索引/修正映射规则）

### 4.6 运行时稳定性（HEADLESS）

基于 validator runtime：

- **崩溃率**：`crashed==true` 或 `runtime_ok==false` 的比例  
- **耗时统计**：p50/p90（用于监控性能退化）

> 如果当前环境 HEADLESS 不稳定：先在 CI/容器固定依赖后再把 runtime 纳入硬门控。

---

## 5. 评估 Harness 实现拆解（建议）

建议把评估拆成 3 个可复用模块：

1) `generate(model, prompt, seed, decode_cfg) -> text`  
2) `extract(text) -> {plan, code}`（格式修复只做“切分/去围栏”，不做语义改写）  
3) `validate(code, prompt_meta) -> validator_json`（调用 `validator/src/cli.js`）

并支持缓存：

- `code_hash` 缓存 validator 结果  
- 生成结果落盘后可重复统计不同指标（无需重跑模型）

---

## 6. 报告与分析（必须包含）

### 6.1 总体指标（与主文档门槛对齐）

自动评估门槛（来自主文档）：

- Pass@1 ≥ 45%（目标 ≥ 50%）  
- Pass@8 ≥ 85%  
- API ≥ 90%  
- 崩溃率 ≤ 5%  
- 结构完整率 ≥ 95%

报告必须同时输出：

- overall（全部 prompt）  
- per-module / per-difficulty / per-ood  
- 与基线对比（Δ）

### 6.2 失败原因 TopN

按失败样本的第一失败原因归类（示例顺序）：

1) parse_failed  
2) lint_failed  
3) api_miss  
4) must_use_miss  
5) structure_missing  
6) runtime_crash  
7) timeout

输出 TopN 并给出代表性样例（保存 prompt_id + 生成文本路径 + validator 报告路径）。

### 6.3 消融与对比（可选但建议）

建议至少比较：

- Base（原始 Qwen2.5-Coder-0.5B）  
- SFT-only  
- GRPO（完整奖励）  

如做消融，可增加：

- 去掉 runtime reward  
- 去掉 API reward  
- 去掉 plan-code 一致性 reward  

要求：

- 同一评估集、同一解码设置  
- 报告中清晰标注 reward 版本与模型版本

---

## 7. 人工评估（200 条，双人盲评）

### 7.1 抽样策略

- 从 300 条评估集中分层抽取 200 条（覆盖模块×难度×OOD）  
- 抽样集冻结：`data/eval/prompts_human_200.jsonl`

### 7.2 评分维度与标准

按主文档维度：

- 功能完整性：0/1/2  
- 代码质量：0/1/2  
- 最佳实践遵循：0/1  

要求：

- 两位评审独立打分（盲评，不展示模型名称）  
- 计算一致性 κ（目标 ≥ 0.6）  
- 平均分目标 ≥ 3.5/5

### 7.3 仲裁与归因

对分歧样本：

- 记录分歧原因（功能理解、可运行性、实现质量）  
- 形成“常见问题清单”，回流到 Prompt 设计/奖励规则/过滤规则

---

## 8. 风险与备选方案

- **HEADLESS 不稳定导致指标波动**：固定依赖版本（建议容器化），并把 runtime 作为独立指标先跑通再纳入门控  
- **validator 误杀/漏杀**：对失败 TopN 做抽检，调整 AST 映射表与规则；评估集冻结，规则迭代需版本化  
- **Pass@k 方差大**：固定 seed 列表；报告中输出置信区间或至少输出每条 prompt 的通过次数分布  
